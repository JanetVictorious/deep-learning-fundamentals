{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification with Keras Sequential model\n",
    "\n",
    "In this notebook we will build a binar classification model utilizing the [Keras Sequential model](https://www.tensorflow.org/guide/keras/sequential_model).\n",
    "\n",
    "The sequential API is well suited for building models with layers that proceed in a sequential manner. This entails that the order of the layers matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from simple_keras import KerasSeqClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "We start by loading the `MNIST` dataset which contains images depicting hand-written numbers from 0-9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "split = ['train', 'test']\n",
    "\n",
    "# Load cats_vs_dogs dataset\n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    name='mnist',\n",
    "    split=split,\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect dataset\n",
    "\n",
    "Start by inspecting the data. A TensorFlow `Dataset` has a lot of properties, for reference see [TensorFlow Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Datasethttps://www.tensorflow.org/api_docs/python/tf/data/Dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print info\n",
    "print(ds_info.splits['train'].num_examples)\n",
    "print(ds_info.splits['test'].num_examples)\n",
    "print(ds_test.element_spec)\n",
    "\n",
    "# Calculate number of classes in label\n",
    "num_classes = ds_info.features['label'].num_classes\n",
    "print(num_classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize some of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_iter = iter(x_train for x_train, _ in ds_train.take(9).cache().repeat())\n",
    "labels_iter = iter(y_train for _, y_train in ds_train.take(9).cache().repeat())\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(next(images_iter).numpy().astype(\"uint8\"))\n",
    "    plt.title(next(labels_iter).numpy().astype(\"uint8\"))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data\n",
    "\n",
    "Before training we need to preprocess the images.\n",
    "\n",
    "We start by normalizing the image pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(image, label):\n",
    "    \"\"\"Normalize image pixels.\"\"\"\n",
    "    return tf.cast(image, tf.float32) / 255.0, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize train and test data\n",
    "ds_train = ds_train.map(normalize, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds_test = ds_test.map(normalize, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print element spec after preprocessing\n",
    "print(ds_train.element_spec)\n",
    "print(ds_test.element_spec)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shapes are the same, `(28, 28, 1)`, but the type has changed, `dtype=tf.float32`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Dataset` API has some nice functionalities for speeding up the traning time.\n",
    "\n",
    "By caching the datasets we we will save some operations (like file opening and data reading) from being executed during each epoch, [reference](https://www.tensorflow.org/guide/data_performance#caching)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up training data\n",
    "\n",
    "First, we take the training data and apply the following steps:\n",
    "* cache it before shuffling for better performance\n",
    "* for true randomness, set shuffle buffer to full dataset size\n",
    "* batch elements of the dataset after shuffling to get unique batches at each epoch\n",
    "* prefetch to increase performace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache train data\n",
    "ds_train = ds_train.cache()\n",
    "\n",
    "# Shuffle data for true randomness and to reduce memory usage\n",
    "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "\n",
    "# Set batch size\n",
    "ds_train = ds_train.batch(32)\n",
    "\n",
    "# Prefetch\n",
    "ds_train = ds_train.prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we take our evaluation data. We do similar steps but skip a few:\n",
    "* we don't need to shuffle the data\n",
    "* caching is done after batching because batches can be the same between epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set batch size\n",
    "ds_test = ds_test.batch(32)\n",
    "\n",
    "# Cache test data\n",
    "ds_test = ds_test.cache()\n",
    "\n",
    "# Prefetch\n",
    "ds_test = ds_test.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "\n",
    "Next we instantiate the model and let it train for a number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate classifier\n",
    "clf = KerasSeqClassifier(input_shape=(28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "clf.call(ds_train=ds_train, ds_eval=ds_test, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.model.evaluate(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
